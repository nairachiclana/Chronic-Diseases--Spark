{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PySpark\n",
    "\n",
    "In this practice we are going to work with PySpark.\n",
    "\n",
    "PySpark is a fast and general-purpose cluster computing system. It provides high-level APIs in Python, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n",
    "\n",
    "In fact, we are going to use PySpark to read a .csv and just change the data types abstractions. There are three different types of data abstractions:\n",
    "\n",
    "- RDD\n",
    "- DataFrame\n",
    "- DataSet\n",
    "\n",
    "But for this practice we are just going to uso two of them, RDD and DataFrame.\n",
    "\n",
    "***Spark RDD APIs – ***An RDD stands for Resilient Distributed Datasets. It is Read-only partition collection of records. RDD is the fundamental data structure of Spark. It allows a programmer to perform in-memory computations on large clusters in a **fault-tolerant** manner. Thus, speed up the task.\n",
    "\n",
    "\n",
    "***Spark Dataframe APIs – ***Unlike an RDD, data organized into named columns. For example a table in a relational database. It is an immutable distributed collection of data. DataFrame in Spark allows developers to **impose a structure** onto a distributed collection of data, allowing higher-level abstraction.\n",
    "\n",
    "# Spark RDD\n",
    "\n",
    "First of all, we set directly on a SparkConf passed to your SparkContext. SparkConf allows you to configure some of the common properties\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "spark_conf = SparkConf()\n",
    "spark_context = SparkContext(conf=spark_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to log into spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = spark_context._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\").setLevel(logger.Level.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we get the values that we are interested in, by this lambda expresion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = spark_context \\\n",
    "        .textFile(file_name)\\\n",
    "        .map(lambda line: line.split(\",\"))\\\n",
    "        .map(lambda list: (list[5], 1))\\\n",
    "        .reduceByKey(lambda x, y: x + y)\\\n",
    "        .sortBy(lambda pair: pair[1])\n",
    "\n",
    "        #textFile -- open the file\n",
    "        #map line: line.split -- Hace un split de cada línea\n",
    "        #map list: list[5], 1 -- Guardamos la posición de topic\n",
    "        #reduceByKey x, y: x + y -- Coge los elementos de la lista y los suma hasta dejar uno\n",
    "        #sortBy pair: pair[1] -- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
